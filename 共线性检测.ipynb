{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 共线性问题的处理\n",
    "## 一. 如何检验共线性\n",
    "### 1.容忍度（Tolerance）\n",
    "容忍度是每个自变量作为因变量对其他自变量进行回归建模时得到的残差比例，大小用1减得到的决定系数来表示，容忍度的值介于0.1和1之间，如果值越小，说明这个自变量与其他自变量间越可能存在共线性问题。\n",
    "### 2.方差膨胀因子（Variance Inflation Factor， VIF）\n",
    "VIF是容忍度的倒数，值越大则共线性问题越明显，通常以10为判断边界。当VIF<10，不存在多重共线性；当10<=VIF<100,存在较强的多重共线性；当VIF > 100时，存在严重多重共线性。\n",
    "### 3.特征值（Enginevalue）\n",
    "对自变量进行主成分分析，如果多个维度的特征值等于0，则可能存在多重共线性问题。\n",
    "### 4.相关性（Corelation）\n",
    "使用相关性辅助判断。\n",
    "## 二.解决共线性问题常用的方法\n",
    "### 1.增大样本量\n",
    "通过增加样本量，来消除由于数据量不足而出现的偶然共线性现象。\n",
    "### 2.岭回归法（Ridge Regression）\n",
    "岭回归分析是一种专用于共线性问题的有偏估计回归方法。它通过放弃最小二乘法的无偏性，以损失部分信息、降低精度为代价来获得更实际和可靠性更强的回归系数。常用于存在较强共线性的回归应用中。\n",
    "### 3.逐步回归法（Stepwise Regression）\n",
    "逐步回归法是每次引入一个自变量并及逆行统计检验，然后逐步引入其他变量，同时对所有变量的回归系数进行检验。若原来引入的变量由于后面的变量的引入而变得不再显著，就将其剔除，逐步得到最优回归方程。\n",
    "### 4.主成分回归（Principal Component Regression）\n",
    "通过主成分分析，讲原始参与建模的变脸转换成少数几个主成分，每个主成分都是原始变量的线性组合，这样也可以一定程度上避免共线性问题。\n",
    "### 5.人工去除\n",
    "结合人工经验，根据回归分析删减自变量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
